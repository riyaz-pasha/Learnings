# ðŸ”¥ 4ï¸âƒ£ Distributed Unique ID Collision Detector

---

## ðŸ§¾ Problem Statement

Multiple services generate IDs and send them to your system:

```
(timestamp, generatedId)
```

### Requirements

Detect if:

> The same 128-bit ID appears more than once within a 1-minute window across the cluster.

### Constraints

* IDs are 128-bit (very large space)
* Memory constrained
* High throughput
* Distributed system
* Only need to detect duplicates within last 1 minute
* False positives may or may not be acceptable (clarify)

---

# ðŸ§  Step 1: Clarify Key Questions

In interview you ask:

1. Are IDs globally random?
2. What is event rate per second?
3. Are false positives acceptable?
4. Do we need to store IDs longer than 1 minute?
5. Strong consistency required?

Assume:

* Very high throughput (millions/sec)
* Memory constrained
* Only 1-minute detection window
* Small false positive rate acceptable

---

# ðŸ§  Step 2: Naive Solution (Impossible at Scale)

Maintain:

```
Set<generatedId>
```

And remove entries older than 1 minute.

Problems:

If 5M IDs/sec:

```
5M Ã— 60 = 300M IDs
```

Each ID 128 bits = 16 bytes minimum
Memory:

```
~ 300M Ã— 16 bytes = ~4.8GB (just raw)
```

Plus overhead â†’ impossible.

Rejected.

---

# ðŸ§  Step 3: Observing Key Property

We only care about:

> Did this ID appear in last 60 seconds?

Not:

* Order
* Count
* Who generated

So this is membership check in sliding window.

This screams:

> Bloom Filter + Time Bucketing

---

# ðŸ§  Step 4: Bloom Filter Refresher

Bloom filter:

* Bit array
* Multiple hash functions
* Probabilistic
* No false negatives
* Possible false positives

Perfect for memory-bounded duplicate detection.

---

# ðŸ§  Step 5: Problem with Single Bloom Filter

If we use single Bloom filter:

We cannot delete expired IDs.

Bloom filter doesn't support deletion (unless Counting Bloom Filter).

But we need 1-minute window.

---

# ðŸ§  Step 6: Time-Bucketed Bloom Filters (Correct Design)

Divide 1 minute into smaller buckets.

Example:

```
60 buckets of 1 second each
```

Maintain:

```
BloomFilter[60]
```

Each bucket stores IDs for that second.

---

### On new event:

1. Determine bucket index:

```
bucket = timestamp % 60
```

2. Before using that bucket:

   * Clear it (since itâ€™s now >60 sec old)

3. Check ID across all 60 buckets

   * If found in any â†’ duplicate
   * Else â†’ insert into current bucket

---

# ðŸ§  Why This Works

We maintain sliding 60-second window.

Memory:

```
60 Ã— bloomFilterSize
```

Instead of storing raw IDs.

Massive memory savings.

---

# ðŸ§  Step 7: Distributed Aspect

If multiple servers process IDs:

We must ensure same ID always checked consistently.

Two options:

---

## Option A: Centralized Store

All nodes write to shared Redis-like store.

Pros:

* Easy correctness

Cons:

* Bottleneck

---

## Option B: Partition by Hash (Better)

Use:

```
hash(generatedId) % N
```

Each partition handles subset of IDs.

This guarantees:
Same ID always goes to same node.

Now dedup is local per partition.

Much more scalable.

---

# ðŸ§  Step 8: False Positive Tradeoff

If Bloom filter says duplicate:

* It might be false positive.

Question:
Is that acceptable?

If system is detecting ID collisions:
False positives = thinking collision happened when it didn't.

Usually acceptable if rate is tiny.

If unacceptable:
Must use Counting Bloom Filter or exact storage (costly).

---

# ðŸ§  Step 9: Memory Calculation Example

Suppose:

* 5M IDs/sec
* 60 sec window = 300M IDs

Target false positive rate: 0.01%

Bloom filter size formula:

```
m = -(n * ln p) / (ln 2)^2
```

Roughly:
~ 10 bits per element for low FP.

Memory:

```
300M Ã— 10 bits â‰ˆ 3 billion bits
â‰ˆ 375MB
```

Instead of 5GB raw.

Massive savings.

---

# ðŸ§  Step 10: Java Sketch (Conceptual)

```java
class CollisionDetector {

    private static final int WINDOW_SECONDS = 60;
    private final BloomFilter[] buckets = new BloomFilter[WINDOW_SECONDS];

    public boolean isDuplicate(long timestamp, String id) {

        int index = (int)(timestamp % WINDOW_SECONDS);

        // reset expired bucket
        buckets[index].clear();

        // check across all buckets
        for (BloomFilter bf : buckets) {
            if (bf.mightContain(id)) {
                return true;
            }
        }

        // insert into current bucket
        buckets[index].put(id);

        return false;
    }
}
```

(Real implementation would optimize cross-bucket checking.)

---

# ðŸ§  Step 11: Optimized Check

Instead of scanning 60 filters each time:

Maintain:

* A master filter that is union of active buckets
* Rebuild periodically

Or:
Use Counting Bloom Filter with TTL per counter.

---

# ðŸ§  Step 12: Edge Cases

1. Clock skew between nodes
2. Out-of-order timestamps
3. System restart (need rebuild strategy)
4. Burst traffic
5. Hash collision behavior

---

# ðŸ§  What Google Is Testing

* Memory-bound sliding window design
* Probabilistic data structures
* Time-bucket rotation
* Partitioning strategy
* False positive tradeoffs
* Scalability reasoning

This is classic L5-level systems thinking.

---

# ðŸ§  Riyaz-Level Interview Framing

You should say:

> "Because we only need duplicate detection within a bounded time window and memory is constrained, we use time-bucketed Bloom filters. We partition by hash of the ID to ensure cluster-wide consistency, and rotate buckets every second to implement sliding window eviction."

That sounds very strong.

---

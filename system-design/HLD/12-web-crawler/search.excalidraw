{"elements":[{"versionNonce":2051141903,"fontFamily":8,"verticalAlign":"top","updated":1756917203773,"opacity":100,"strokeColor":"#1e1e1e","strokeStyle":"solid","angle":0,"text":"Search","autoResize":true,"fontSize":20,"id":"sY2ZeG4sSRdXORaI8dgMU","backgroundColor":"transparent","isDeleted":false,"groupIds":[],"fillStyle":"solid","strokeWidth":2,"originalText":"Search","version":8,"type":"text","locked":false,"lineHeight":1.25,"y":145,"height":25,"textAlign":"left","x":681,"width":66,"seed":1735087663,"roughness":1,"index":"a0"},{"versionNonce":1809978351,"fontFamily":8,"verticalAlign":"top","updated":1756923247349,"opacity":100,"strokeColor":"#1e1e1e","angle":0,"strokeStyle":"solid","text":"Crawling and parsing are the first two stages of a web search engine's data ingestion pipeline. \nThe **crawling flow** discovers and downloads web pages, while the **parsing flow** extracts and cleans the content for the next stages.\n\n---\n\n### **Crawling Flow**\n\nThe crawling process begins with a set of **seed URLs** stored in a queue. \nA **URL Manager service** pushes these URLs to a **distributed message queue** (like Kafka). \n\n1.  **Crawler Workers** consume messages from the queue.\n2.  Each worker performs a **DNS lookup** to resolve the domain to an IP address.\n3.  It then fetches the site's **`robots.txt`** file to check for crawl rules and delays.\n4.  The worker makes an **HTTP request** to download the raw HTML content.\n5.  The raw HTML is saved to **Object Storage** (e.g., S3), and a `crawl.raw` event is published to a message queue,\n    carrying only the storage location (e.g., an S3 key). This decouples the crawling from the parsing stage.\n\n---\n\n### **Parsing Flow**\n\nAfter the raw data is stored, the parsing process begins.\n\n1.  A **Parsing Worker** consumes the `crawl.raw` event from the queue.\n2.  It retrieves the raw HTML from Object Storage.\n3.  The parser uses an **HTML parsing library** to build a **Document Object Model (DOM)** tree.\n4.  It extracts two key types of data:\n    * **Content:** The visible text, titles, and metadata are extracted and cleaned.\n    * **Links:** New URLs are found from `<a>` tags and are sent back to the URL Manager to be added to the crawl queue for future crawling.\n5.  The cleaned content is stored in a **Document Store** (a database), and a `crawl.parsed` event is published to a message queue. \n    This event contains the **document ID**, signaling to the next stage (the indexer) that a new document is ready for processing.","autoResize":true,"fontSize":15.789253050920038,"id":"alfbCA4hjEExwBv2aRQnG","backgroundColor":"transparent","groupIds":[],"fillStyle":"solid","isDeleted":false,"strokeWidth":2,"originalText":"Crawling and parsing are the first two stages of a web search engine's data ingestion pipeline. \nThe **crawling flow** discovers and downloads web pages, while the **parsing flow** extracts and cleans the content for the next stages.\n\n---\n\n### **Crawling Flow**\n\nThe crawling process begins with a set of **seed URLs** stored in a queue. \nA **URL Manager service** pushes these URLs to a **distributed message queue** (like Kafka). \n\n1.  **Crawler Workers** consume messages from the queue.\n2.  Each worker performs a **DNS lookup** to resolve the domain to an IP address.\n3.  It then fetches the site's **`robots.txt`** file to check for crawl rules and delays.\n4.  The worker makes an **HTTP request** to download the raw HTML content.\n5.  The raw HTML is saved to **Object Storage** (e.g., S3), and a `crawl.raw` event is published to a message queue,\n    carrying only the storage location (e.g., an S3 key). This decouples the crawling from the parsing stage.\n\n---\n\n### **Parsing Flow**\n\nAfter the raw data is stored, the parsing process begins.\n\n1.  A **Parsing Worker** consumes the `crawl.raw` event from the queue.\n2.  It retrieves the raw HTML from Object Storage.\n3.  The parser uses an **HTML parsing library** to build a **Document Object Model (DOM)** tree.\n4.  It extracts two key types of data:\n    * **Content:** The visible text, titles, and metadata are extracted and cleaned.\n    * **Links:** New URLs are found from `<a>` tags and are sent back to the URL Manager to be added to the crawl queue for future crawling.\n5.  The cleaned content is stored in a **Document Store** (a database), and a `crawl.parsed` event is published to a message queue. \n    This event contains the **document ID**, signaling to the next stage (the indexer) that a new document is ready for processing.","type":"text","version":95,"locked":false,"lineHeight":1.25,"y":290,"height":611.8335557231536,"textAlign":"left","x":302,"seed":1801467759,"width":1215.7724849208453,"index":"a1","roughness":1},{"versionNonce":1957040303,"fontFamily":8,"verticalAlign":"top","updated":1756923935939,"opacity":100,"strokeColor":"#1e1e1e","angle":0,"strokeStyle":"solid","text":"### Indexing Flow\n\n- Index service consumes “crawl.parsed” events from the message queue.\n  {url, documentId}\n\n- The indexer retrieves the document content from the database.\n- It then breaks the content down into tokens (individual words).\n- Removes punctuations. Replaces & with “and” etc.\n- Converts all tokens\/words into lowercase.\n- Removes stop words such as “the”, “a”, “is”, “and”.\n- The reduces tokens into root forms. Knowns as Stemming\/Lemmatization.\n  For example\n    running, ran, runs -> run\n\n- Then it creates a map of word to freq and positions in local.\n  { \n    run: freq=3, positions=[11,33,45],\n    slow: freq=1, positions=[12],\n  }\n- Then it updates this document frequences in the index.\n   “run” -> {\n            doc1: freq=3, positions=[11,33,45],\n            doc3: freq=1, positions=[21],\n            }\n    “slow” -> {\n            doc1: freq=1, positions=[12],\n            }\n    ","autoResize":true,"fontSize":20,"id":"9VutiYHHSFxqM0hhVVTZo","backgroundColor":"transparent","groupIds":[],"fillStyle":"solid","isDeleted":false,"strokeWidth":2,"originalText":"### Indexing Flow\n\n- Index service consumes “crawl.parsed” events from the message queue.\n  {url, documentId}\n\n- The indexer retrieves the document content from the database.\n- It then breaks the content down into tokens (individual words).\n- Removes punctuations. Replaces & with “and” etc.\n- Converts all tokens\/words into lowercase.\n- Removes stop words such as “the”, “a”, “is”, “and”.\n- The reduces tokens into root forms. Knowns as Stemming\/Lemmatization.\n  For example\n    running, ran, runs -> run\n\n- Then it creates a map of word to freq and positions in local.\n  { \n    run: freq=3, positions=[11,33,45],\n    slow: freq=1, positions=[12],\n  }\n- Then it updates this document frequences in the index.\n   “run” -> {\n            doc1: freq=3, positions=[11,33,45],\n            doc3: freq=1, positions=[21],\n            }\n    “slow” -> {\n            doc1: freq=1, positions=[12],\n            }\n    ","type":"text","version":990,"locked":false,"lineHeight":1.25,"y":1166.3768354796694,"height":700,"textAlign":"left","x":308.15928386963196,"seed":1755104847,"width":781,"index":"a2","roughness":1},{"versionNonce":1073227183,"fontFamily":8,"verticalAlign":"top","updated":1756924830036,"opacity":100,"strokeColor":"#1e1e1e","angle":0,"strokeStyle":"solid","text":"### Search Service Flow\n\n- User submits a search query from front end app.\n- Search Service receives the query and applies the exact same text processing and analysis\n  steps used during indexing.\n    - tokenizaiton\n    - lowercasing\n    - normalization (punctuation removal)\n    - stop word removal\n    - stemming\n\n- Then processed query tokens are looked up in the inverted index.\n- Retrieves documents ids associated with each token.\n“run” -> {doc1, doc3}\n“slow” -> {doc1}\n\n- Then these documents are ranked using some algorithm (Term Frequency- Inverse Document Frequency).\n- TF - How often term appears in the document. The more the count the more the relavance.\n- IDF - How Unique a term is across all the documents fetched. terms slow is more valuable than term “how”.\n\n- Then it aggregates and ranks the response and returns to the user.\n","autoResize":true,"fontSize":20,"id":"y9tee8PUbdupkhBrh4Nfk","backgroundColor":"transparent","groupIds":[],"fillStyle":"solid","isDeleted":false,"strokeWidth":2,"originalText":"### Search Service Flow\n\n- User submits a search query from front end app.\n- Search Service receives the query and applies the exact same text processing and analysis\n  steps used during indexing.\n    - tokenizaiton\n    - lowercasing\n    - normalization (punctuation removal)\n    - stop word removal\n    - stemming\n\n- Then processed query tokens are looked up in the inverted index.\n- Retrieves documents ids associated with each token.\n“run” -> {doc1, doc3}\n“slow” -> {doc1}\n\n- Then these documents are ranked using some algorithm (Term Frequency- Inverse Document Frequency).\n- TF - How often term appears in the document. The more the count the more the relavance.\n- IDF - How Unique a term is across all the documents fetched. terms slow is more valuable than term “how”.\n\n- Then it aggregates and ranks the response and returns to the user.\n","type":"text","version":932,"locked":false,"lineHeight":1.25,"y":2021.4930464171694,"height":550,"textAlign":"left","x":314.15928386963196,"seed":956365583,"width":1177,"index":"a3","roughness":1}],"source":"https:\/\/excalidraw.com","appState":{"viewBackgroundColor":"#ffffff"},"type":"excalidraw","version":2,"files":{}}